{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oulu_NLPTM_TwitterBrexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    tweet_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for tweet in f:\n",
    "            tweet_list.append(tweet.strip())\n",
    "            \n",
    "    print(f\"loaded tweets from {filename}\")\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conservative_tweets = load(\"conservative_tweets_preprocessed.txt\")\n",
    "labour_tweets = load(\"labour_tweets_preprocessed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting top 10 frequent words\n",
    "def top10(tweets):\n",
    "    list_of_words = []\n",
    "    for tweet in tweets:\n",
    "        words = tweet.split()\n",
    "        list_of_words.append(words)\n",
    "    flat_list = [item for sublist in list_of_words for item in sublist]\n",
    "\n",
    "    word_counts = Counter(flat_list)\n",
    "    return word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_conservative = top10(conservative_tweets)\n",
    "top10_labour = top10(labour_tweets)\n",
    "print('TOP10 words by conservatives: ')\n",
    "for i, word in enumerate(top10_conservative):\n",
    "    print(str(i+1) + \". wordstem:'\" +  str(word[0]) + \"' occurences: \" + str(word[1]))\n",
    "\n",
    "print()\n",
    "print('TOP10 words by labour: ')\n",
    "for i, word in enumerate(top10_labour):\n",
    "    print(str(i+1) + \". wordstem:'\" +  str(word[0]) + \"' occurences: \" + str(word[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_conservative_wordset = set([word[0] for word in top10_conservative])\n",
    "top10_labour_wordset = set([word[0] for word in top10_labour])\n",
    "\n",
    "jaccard_index_top10 = nltk.jaccard_distance(top10_conservative_wordset, top10_labour_wordset)\n",
    "\n",
    "print('Jaccard distance based on TOP10 most frequent words: ' + str(jaccard_index_top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "    \n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "def get_topic_words(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    topic_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_words.extend([words[i]for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer_conservative = CountVectorizer()\n",
    "count_vectorizer_labour = CountVectorizer()\n",
    "# Fit and transform the processed titles\n",
    "count_data_conservative = count_vectorizer_conservative.fit_transform(conservative_tweets)\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data_labour = count_vectorizer_labour.fit_transform(labour_tweets)\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "#print(\"conservative\")\n",
    "#plot_10_most_common_words(count_data_conservative, count_vectorizer_conservative)\n",
    "\n",
    "#print(\"labour\")\n",
    "#plot_10_most_common_words(count_data_labour, count_vectorizer_labour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 5\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda_conservative = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda_conservative.fit(count_data_conservative)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "#print(\"Topics found via LDA for conservative tweets:\")\n",
    "#print_topics(lda_conservative, count_vectorizer_conservative, number_words)\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda_labour = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda_labour.fit(count_data_labour)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "#print(\"Topics found via LDA for labour tweets:\")\n",
    "#print_topics(lda_labour, count_vectorizer_labour, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words_conservative = set(get_topic_words(lda_conservative, count_vectorizer_conservative, number_words))\n",
    "topic_words_labour = set(get_topic_words(lda_labour, count_vectorizer_labour, number_words))\n",
    "\n",
    "#print(topic_words_conservative)\n",
    "#print(topic_words_labour)\n",
    "\n",
    "jaccard_index_topics = nltk.jaccard_distance(topic_words_conservative, topic_words_labour)\n",
    "\n",
    "print('Jaccard distance for topic words: ' + str(jaccard_index_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
